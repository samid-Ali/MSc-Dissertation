---
title: "A SVAR Analysis of the Elasticity of GDP to Fiscal Shocks "
author: "Samid Ali"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
    citation_package: biblatex
bibliography: references.bib
csl: harvard-kings-college-london.csl
documentclass: article
header-includes:
  - \usepackage{biblatex}
  - \addbibresource{references.bib}
---

```{r setup, include=FALSE}


# library(knitr)
# library(stargazer)
# library(clipr)
#library(kableExtra) 

library(ggplot2)
library(knitr)
library(ivreg)
library(ggdag)
library(data.table)
library(dplyr)
library(tidyr)
library(stargazer)
library(clipr)
library(tibble)
library(lubridate)
library(boot)
library(seasonal)



lapply(c("ggplot2", "dplyr", "data.table", "lubridate", "janitor", "broom", "tibble", "tidyr", "aTSA", "vars"),
       require, character.only = TRUE)

# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)



```



\newpage

<!-- \frontmatter -->




## Abstract

\newpage

## Table of Figures
\listoffigures



## Table of Tables
\listoftables

\newpage

<!-- \mainmatter -->


# Introduction

- motivate
- research gap / contribution
- summarise/ outline structure

This paper proceeds as follows, Section 2 provides a review of the related literature, focusing on the costs of high indebtedness and evidence on the effects of fiscal consolidation. We also provide a synthesis of the methods used to estimate fiscal multipliers, highlighting the use of Structural Vector Autoregressions (SVARs). This provides us with sufficient context and foundations to proceed with section 3  where we introduce the data and reduced form model. Here the Vector Error Correction Model (VECM) representation is considered as a generalisation of the VAR model which can yield efficiency gains when properly specified. We considered the merits of this representation and perform pre-testing of the specification. Following this, in Section 4 we introduce the identifying assumptions to allow us to recover the structural shocks. With our model now defined, Section 5 presents the results of this paper, focusing on the Impulse Response Function (IRF) and Forecast Error Variance Decomposition (FEVD). Overall, the IRFs of interest are statistically and practically significant; in Section 6 we consider robustness checks such as parameter stability and a lower lag lenght to see whether the results are sensitive to the specification. Finding that the result of low fiscal multipliers in the UK are robust to the specification, Section 7 proceeds with a discussion of policy implications. In Section 8 we then contextualize the results of this study and scope for future research, so that the results can be appropriately appreciated. Section 9 concludes.

# Literature Review


## Costs of high indebtedness

Warmedinger, Checherita-Westphal, and De Cos (2015) emphasise the importance of public debt sustainability for ensuring macroeconomic stability. Recent economic crises have been met with government intervention, leading to further strains on public finances. For instance, Sutherland, Hoeller, and Merola (2012) draw attention to the fiscal challenges facing countries following the Global Financial Crisis (GFC). They also note that gross government debt has exceeded 100% of GDP for the OECD as an aggregate. These concerns have been exacerbated following the Covid pandemic where governments implemented fiscal measures to mitigate the economic costs of the pandemic (IMF, 2023). 

Makin and Layton (2021) highlight that governments must employ fiscal responsibility to protect their economies from the risks that high indebtedness exposes them to. There are several mechanisms through which these risks may be presented. High indebtedness reduces the ability of countries to effectively respond counter cyclically to economic shocks. Additionally, when the interest rate exceeds the growth rate, the economy is unable to sustain the cost of growth. While Blanchard (2019) suggests that the low interest environments relative to growth seen recently have been persistent historically (which would suggest that debt could be self-stabilising due to economic growth outpacing the cost of debt), he also argues that there would still be a welfare cost in such a scenario. This is because Blanchard argues that capital accumulation reduces in response to high indebtedness. This aligns with the analysis of Alesina and Ardagna (1998) that "Wealth rises when future tax burdens decline": economic agents anticipate greater tax burdens due to the excessive levels of public indebtedness, and consequently confidence worsens leading to lower consumption and investment. 


Kumar and Woo (2015) provide further evidence of the cost of public indebtedness, finding that greater indebtedness is associated with lower economic growth. They find noticeable non-linearities in this result, with the most severe effect occurring when public indebtedness exceeds 90% of GDP. Ilzetzki, Mendoza, and Vegh (2013) suggest that shocks to government speding can have strong negative effects on output at debt levels of as little as 60% of GDP. Recalling that public indebtedness has exceeded 100% in the OECD overall (Sutherland, Hoeller, and Merola ,2012), these results will be of particular interest for advanced economies. This highlights the importance of fiscal consolidation  to ensure the long-term resilience of the economy. The IMF (2024) argue that economies should rebuild their fiscal buffers to reduce debt vulnerabilities, proposing that fiscal adjustments would need to be in the region of 4% of GDP. Sutherland, Hoeller, and Merola (2012) have argued for more aggressive adjustments, exceeding 5% of GDP in order to bring debt down to 50% of GDP. This figure would require the UK to halve its current debt levels (ONS, 2025). Thus, achieving this objective would require significant fiscal adjustments. **The next subsection synthesises the literature on fiscal consolidation and its effects.**



## Fiscal Consolidaton

While the importance of fiscal consolidation has been highlighted, it is crucial that these measures are not at the expense of the broader economy. By investigating forecast errors for a sample of European countries, Blanchard and Leigh (2013) find that larger anticipated fiscal consolidation was associated with lower growth. This result was interpreted as due to the fiscal multipliers being greater than anticipated by forecasters. Consequently, fiscal tightening would have further dampened demand, meaning that improvements to government finances through fiscal consolidation would be offset by reduced growth. Gechert, Horn, and Paetz (2017) adopt a similar methodology, finding that austerity measures in the Euro Area deepened the GFC, with large persistence in their results. This suggests that, even in the long run, poorly implemented fiscal adjustments could be counterproductive. Fatas and Summers (2018) extend this research, investigating the long-term effects that fiscal adjustments have had on GDP. Their analysis suggests that fiscal consolidations have failed to lower the debt-to-GDP ratio due to a hysteresis effect of contractionary fiscal policy. This research underscores the need to effectively quantify fiscal multipliers, allowing policymakers to better understand the potential trade-offs between various economic objectives.

Despite this, the literature has struggled to gain consensus on the size, and even the sign of fiscal multipliers (Caldara and Kamps, 2008). Similarly, Ramey (2016) notes that there has been conflicting evidence in the literature as to which fiscal policy has the greater multiplier, noting that the lack of precision of estimates have made it difficult to make meaningful comparisons. The next subsection focuses on the methodologies used to estimate fiscal multipliers. Before proceeding, we discuss further evidence in the literature of heterogeneity in fiscal multipliers.

Canzoneri et al (2016) suggest that fiscal multipliers can vary throughout the business cycle, with greater effects being observed during recessions. Auerbach and Goronichenko (2012) also provide evidence of the greater effectiveness of fiscal policy during recessions. Furthermore, they note that there can be heterogeneity of multipliers within specific fiscal instruments, noting that military spending had the greatest multipliers. Adding to the literature on state dependent multipliers, Ghassibe and Zanetti (2022) note that the cause of a recession determines the extent of fiscal multipliers. While they report government spending to have larger multipliers during demand-driven recessions, they note that these can be ineffective when the recession is driven by supply side factors. On the other hand, they note that policies such as reductions in income taxes can be effective during supply-driven downturns, as they can help to boost aggregate supply.

Ilzetzki, Mendoza, and Vegh (2013) suggest that the heterogeneity in the estimates reported in the literature can be attributed to differences in structural characteristics of the economy considered. Key characteristics that the authors emphasise include: the level of development, public indebtedness, and openness to trade.  


Alesina, Favero, and Giavazzi (2015) compare multipliers due to spending and tax adjustments. They find that tax-based adjustments have more severe effects than those based on adjustments to expenditure. They attribute this result to business confidence. They argue that sentiment recovers quickly in response to government spending cuts, however increasing revenue does not address the root of the problem and thus confidence struggles to recover. Similarly, they attribute confidence to their finding that adjustments as less effective when subject to reversals: this creates noise when future adjustments are made, agents are unable to ascertain the reliability of announcements, and consequently their effectiveness decreases. This reinforces the importance of research to better understand the fiscal multiplier for different policy instruments, particularly as this may vary across countries and over time. 

While Baldacci, Gupta, and Granados (2012) find that successful debt consolidations are most likely when based on cuts to current expenditures, they note that raising tax revenue is also needed when the scale of fiscal consolidation needed is large. 
They use duration analysis to support their analysis, investigating hpw 

## Synthesis of Methodology

Ramey (2019) surveys the literature on fiscal multipliers, noting the prominence o

among the empirical approaches for estimating the effects of fiscal policy are SVARs and DSGE models.

In this review Ramey highlights variation in estimates for the fiscal multipliers, with ranges for government spending varying from 0.3 to 2. Ramey highlights that tax multipliers are typically negative, however with much greater variability, reporting a range from 0 for corporate income tax in an estimated DSGE model, to -5 in for overall taxes in an SVAR identified using sign restrictions. 

DSGE

**Christiano, Eichenbaum, and Rebelo (2011) investigate the size of the output multiplier for government spending using a Dynamic Stochastic General Equilibrium (DSGE) model. Through this model, they show that the multiplier for government spending can be greater than 1 when the zero bound binds. **

**Barnichon, Derbotoli, and Matthes (2022)  find asymmetric effects of shocks to government spending: while they suggest the contractionary multiplier can exceed 1 during recessions, they find the expansionary multiplier to always be less than 1. The authors employ the Functional Approximations of Impulse Responses methodology (Barnichon and Matthes, 2018)**

**Woodford (2011) also use a DSGE model, finding that wage rigidities and stiky prices allow for larger multipliers than anticipated in a neoclassical model. They also find that the multiplier can exceed 1 wjem the zero bound binds, attributing this to the ability of government spending to fill the output gap when interest rates cannot be lowered.**

**Zubairy (2014) estimate a DSGE model, finding a crowding in effect of government spending, peaking at impact.** 

Capek and Cuasera (2020) simulated 20 million fiscal multipliers, highlighting how methodological choices contribute to the heterogeneity in estimates of fiscal multipliers prevalent in the literature. Consequently, they advocate for explicitly outlining modelling choices and assumptions. Similarly, Gechert (2017) provides a synthesis of the methodologies used to estimate fiscal multipliers, highlighting competing definitions for the fiscal multiplier and possible issues in its estimation. Among these issues, Gechert (2017) highlights potential omitted variable bias in the VAR model. When such variables are excluded from the VAR model, Structural VAR (SVAR) tools such as IRFs may lose their interpretation due to contamination with the unobeserved effect.



Structural Vector Autoregressions (SVARs) have been prominent in the literature to estimate fiscal multipliers. Various approaches to identification have been used, with XXX (YYYY) noting that after accounting for the empirical specification, the competing identifying approaches have little effect on the estimated multipliers. Blanchard and Perotti (2002) pioneered this strand of the research, leveraging methodologies previously popularised by the monetary economics. To identify their SVAR, Blanchard and Perotti leverage instituional information. They provide a definition for the fiscal variables and highlight that government expenditure is predetermined within a quarter. 
Recursive measures to identfication have been employed by Fatas and Mihov (YYYY) and Fernandex (2008). Fernandez argues that
Uhlig and Mountford (200Y) apply restrictions on the signs of the impulse response functions. 
Caldara and Kamps (2008) reviews the literature on SVAR identification. Caldara and Kamps (2017) introduce a new approach for identification.

Kim et al (2021) extend the recursively identified SVAR, finding evidence of cointegrating relationships ^[The authors rely on Johansen's cointegration tests although do not appear to consider finite sample bias in the test, which we highlight later in this paper. Nevertheless, given the use of monthly data, their sample size is greater than this paper's leading to a lower finite sample scaling factor. Their tests for cointegrating ranks appear to remain valid after applying the finite sample correction. ]. Consequently, they estimate the reduced form by a Vector Error Correction Model (VECM), attempting to take advantage of the information on the model's long run dynamics contained within the cointegrating relationships. 

Auerbach and Gorodnichenko (2012) explore state-dependent fiscal multipliers using regime-switching models. They find evidence of substantial differences in multipliers between recessionary and expansionary periods. 

*Add DSGE lit for context*






```{r dataProc}

df <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/GDP Real.csv", skip = 1)


# Filter the data frame to exclude rows where the column 'title' matches any of the specified values


filtered_df <- df %>%
  # Keep only the quarterly data
  filter(nchar(CDID) == 7 & substr(CDID, 6, 6) == "Q") %>%
  # Select relevant columns and rename them
  dplyr::select(CDID, Deflator = L8GG, GDP = CAGQ) %>%
  # dplyr::select(CDID, Deflator = L8GG, GDP = ABMI) %>%
  # Create new columns and convert types
  mutate(
    Year = as.numeric(substr(CDID, 1, 4)),
    Quarter = substr(CDID, 6, 7),
    Q = as.numeric(substr(CDID, 7, 7)),
    Deflator = as.numeric(Deflator),
    GDP = as.numeric(GDP)
  ) %>%
  # Filter by year (can modify for testing)
  filter(Year >= 1987)


 
# fiscal_raw <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Fiscal Data.csv", skip = 1)

fiscal_raw <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Fiscal Data General.csv")

fiscal_proc <- fiscal_raw %>% 
  dplyr::select(Date_ID = Identifier, Revenue = Revenue, Expenditure = Expenditure) %>% 
  mutate(Year = as.numeric(gsub("\\D", "", Date_ID)),
         Period = gsub("\\d{4}", "", Date_ID))  %>% 
  mutate(
    Q = case_when(
      Period == "Jan to Mar " ~ 1,
      Period == "Apr to Jun " ~ 2,
      Period == "Jul to Sep " ~ 3,
      Period == "Oct to Dec " ~ 4
      ),
    Unique_Period = Year +(Q/4)
   
  ) %>% 
  # Convert to numeric 
  mutate(Revenue = as.numeric(gsub(",", "", Revenue) ),
         Expenditure = as.numeric(gsub(",", "", Expenditure )))

# fiscal_proc <- fiscal_raw %>% 
#   dplyr::select(Date_ID = Transaction, Revenue = OTR, Expenditure = OTE) %>% 
#   subset(Date_ID !=  "Dataset identifier code" & Date_ID != "Identifier") %>% 
#   mutate(Year = as.numeric(gsub("\\D", "", Date_ID)),
#          Period = gsub("\\d{4}", "", Date_ID))  %>% 
#   mutate(
#     Q = case_when(
#       Period == "Jan to Mar " ~ 1,
#       Period == "Apr to Jun " ~ 2,
#       Period == "Jul to Sep " ~ 3,
#       Period == "Oct to Dec " ~ 4
#       ),
#     Unique_Period = Year +(Q/4)
#    
#   ) %>% 
#   # Convert to numeric and multiply by 1 million so values as these will later be made into per capita terms
#   mutate(Revenue = as.numeric(gsub(",", "", Revenue) ),
#          Expenditure = as.numeric(gsub(",", "", Expenditure )))

# join GDP deflator and GDP data

population <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Population.csv", 
                    skip = 4, 
                    header = TRUE) %>% 
  subset(`Country Name` == "United Kingdom") %>% 
  t() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Year") %>% 
  rename(Population = V1 ) %>% 
  filter(grepl("^\\d{4}$", Year)) %>% 
  mutate(Year = as.numeric(Year),
         Population = as.numeric(Population))

Interest_SR <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/3 month Rate.csv") %>% 
  mutate(Date = dmy(Date),
         month = month(Date),
         Year = year(Date),
         Q = case_when(
           month %in% 1:3 ~ 1,
           month %in% 4:6 ~ 2,
           month %in% 7:9 ~ 3,
           month %in% 10:12 ~ 4
         )) %>% 
  group_by(Year, Q) %>% 
  summarize(mean_SR_Rate = mean(Rate, na.rm = TRUE))



SONIA <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Bank of England  Database.csv") %>% 
  mutate(Date = dmy(Date),
         month = month(Date),
         Year = year(Date),
         Q = case_when(
           month %in% 1:3 ~ 1,
           month %in% 4:6 ~ 2,
           month %in% 7:9 ~ 3,
           month %in% 10:12 ~ 4
         )) %>% 
  group_by(Year, Q) %>% 
  summarize(mean_SONIA = mean(SONIA, na.rm = TRUE))

Policy_Rate <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Policy Rate.csv") %>%
  mutate(Date = parse_date_time(`Date Changed`, orders = "dmy"),
         Q = quarter(Date),
         Year = year(Date)) %>%
  group_by(Year, Q) %>%
  summarise(mean_SR_Rate = mean(Rate, na.rm = TRUE), .groups = "drop") %>%
  complete(Year = full_seq(Year, 1), Q = 1:4) %>%  # Ensure all Year-Quarter combinations
  arrange(Year, Q) %>%
  fill(mean_SR_Rate, .direction = "down")  # Fill missing rates by propagating the previous value

Exports <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Exports.csv") %>% 
  mutate(Date = dmy(Month),
         month = month(Month),
         Year = year(dmy(Month)),
         Q = case_when(
           month %in% 1:3 ~ 1,
           month %in% 4:6 ~ 2,
           month %in% 7:9 ~ 3,
           month %in% 10:12 ~ 4
         )) %>% 
  group_by(Year, Q) %>% 
  summarize(Exports = sum(Exports, na.rm = TRUE))

ERI <- fread("D:/Samid work/University/KCL - Econ and Policy/Dissertation/Data/Boe ERI.csv") %>% 
  mutate(
         month = month(Date),
         Year = year(dmy(Date)),
         Q = case_when(
           month %in% 1:3 ~ 1,
           month %in% 4:6 ~ 2,
           month %in% 7:9 ~ 3,
           month %in% 10:12 ~ 4
         )) %>% 
  rename(ERI = Value)


data <- fiscal_proc %>% 
  left_join(filtered_df, by = c("Q" = "Q", "Year" = "Year")) %>% 
  left_join(Interest_SR, by = c("Q" = "Q", "Year" = "Year")) %>% 
  left_join(Exports, by = c("Q" = "Q", "Year" = "Year")) %>% 
  left_join(population, by = c("Year" = "Year")) %>% 
  left_join(ERI, by = c("Q" = "Q", "Year" = "Year")) %>%
# Convert variables to per capita, note revenue, expenditure, and GDP are in £ million so need to multiply. Doing this
  mutate(RevenuePerCapita = (Revenue *10^6) /Population,
         ExpenditurePerCapita = (Expenditure *10^6) /Population,
         GDPPerCapita = (GDP *10^6) /Population) %>% 
  #  Create scaling factor to convert from nominal to current prices
  mutate(real_sf = Deflator /104.4151) %>%  # Divide by current deflator rate (104.8583) as GDP is measured in prices as of ... This allows for better comparability.
  # Create real fiscal variables
  mutate(Revenue_real = Revenue * real_sf,
         Expenditure_real = Expenditure * real_sf) %>% 
  
  # Seasonal Adjustment of data using X-13ARIMA-SEATS
   mutate(Revenue_SA = final(seas(ts(Revenue_real, start = min(Year), frequency = 4))),
         Expenditure_SA = final(seas(ts(Expenditure_real, start = min(Year), frequency = 4))),
         GDP_SA = final(seas(ts(GDP, start = min(Year), frequency = 4))))  %>%
 # convert back to numeric
     mutate(
    Revenue_SA = as.numeric(Revenue_SA),
    Expenditure_SA = as.numeric(Expenditure_SA),
    GDP_SA = as.numeric(GDP_SA)
    
  ) %>%
# Convert variables (except interest rate) to logs
  # Note multiplying by 10^6 for variables that are defined in £millions
  mutate(log_revenue = log(Revenue_SA *10^6),
         log_expenditure = log(Expenditure_SA *10^6),
         log_GDP = log(GDP_SA *10^6),
         log_deflator = log(Deflator),
         log_ERI = log(ERI),
         log_exports = log(Exports *10^6)) %>% 
  mutate(
    dif_log_revenue = log_revenue - lag(log_revenue),
    dif_log_expenditure = log_expenditure - lag(log_expenditure),
    dif_log_GDP = log_GDP - lag(log_GDP),
    dif_log_deflator = log_deflator - lag(log_deflator),
    dif_log_ERI = log_ERI - lag(log_ERI),
    dif_interest_rate = mean_SR_Rate - lag(mean_SR_Rate),
    dif_log_exports = log_exports - lag(log_exports)
  )


         
model_data2 <- data %>%
  dplyr::select(Year, CDID, log_expenditure, mean_SR_Rate, log_GDP, log_ERI, log_revenue,  log_deflator)

# model_data2 <- data %>%
#   dplyr::select(Year, CDID, log_expenditure, mean_SR_Rate, log_GDP, log_exports, log_revenue,  log_deflator)


# model_data <- data %>%
#   dplyr::select(Year, CDID, dif_log_expenditure, dif_interest_rate, dif_log_exports, dif_log_GDP, dif_log_revenue,  dif_log_deflator)

model_data <- data %>%
dplyr::select(Year, CDID, dif_log_expenditure, dif_interest_rate, dif_log_GDP, dif_log_ERI, dif_log_revenue, dif_log_deflator)



# Adding Exports

```





```{r Stationarity Testing}

# Load required packages

# library(dplyr)
# library(purrr)
# library(tseries)

# Select relevant variables (excluding Year and CDID)
# vars_to_test <- model_data2 %>% 
#   dplyr::select(-Year, -CDID)
# 
# # Apply ADF test to each variable
# adf_results <- map(vars_to_test, ~adf.test(.x))
# 
# # Extract p-values and test statistics
# adf_summary <- map_df(adf_results, function(res) {
#   tibble(
#     statistic = res$statistic,
#     p_value = res$p.value
#   )
# }, .id = "variable")
# 
# # View results
# print(adf_summary)
# 
# # Apply KPSS test and extract results
# kpss_summary <- map_df(names(vars_to_test), function(var_name) {
#   series <- vars_to_test[[var_name]]
#   test <- ur.kpss(series, type = "mu")
#   
#   tibble(
#     variable = var_name,
#     statistic = test@teststat,
#     critical_10pct = test@cval["10pct"],
#     critical_5pct  = test@cval["5pct"],
#     critical_1pct  = test@cval["1pct"],
#     stationary_5pct = test@teststat < test@cval["5pct"]
#   )
# })
# 
# # View results
# print(kpss_summary)

```







# Econometric Methodology

Despite the literature highlighting possible importance of state dependence, we focus on a linear VAR model. Reason is that little research has been performed on mulitpliers for the UK, scope is to ...

This research employs a structural vector autoregressive (SVAR) approach to estimating fiscal multipliers. In this section we outline the empirical methodology, as well as the data. We use quarterly data from 1987:1 to 2023:3, modelling this using a six-dimensional VAR at with a lag length of 4. This lag order follows Blanchard and Perroti (2002), and suggests that past values of the endogenous variables continue to have an effect for up to a year. While statistical procedures were not used to determine this lag order, it has an econometric rationale: Kilian and Lutkepohl (2016) note that due to poor finite sample properties of lag selection techniques, it may be appropriate to impose a fixed lag length. Particularly when concerned with impulse response analysis, the risks of underestimating the lag length exceed those of using a larger order which may better reflect the dynamics of the data at the cost of inefficient estimates. Given these considerations, and the degrees of freedom restrictions imposed by the sample size, the lag order of 4 was determined.  

Regarding the endogenous variables included in the VAR model, Blanchard and Perotti (2002) investigate the effects of fiscal shocks using a three-dimensional VAR model consisting of GDP, government expenditure, and government revenue. While such a model could be used to estimate the effects of fiscal shocks, Gechert (2017) highlights the potential issues of omitted variable bias. Therefore, we augment the model to include also a short term interest rate, the GDP deflator rate, and and exchange rate index (ERI). These variables are included to account for the effects of monetary policy, price levels, and interactions with the rest of the world respectively. Consequently, the impulse response functions reported later are better interpreted as the response of GDP to the fiscal variables, ceteris paribus. 


The fiscal variables are sourced from the ONS and defined following the European System of Accounts (ESA, 2010).  The data is available at the central, local, and general levels, the ESA defines general government as encompassing both central and local levels. Therefore the general level is used for this analysis so that the multipliers represent the full effects of fiscal policy. In further detail, 
government expenditure is defined as the outflows associated with government activities, including consumption, investment, and transfers. The inflows to the government, government revenue, consists of receipts net of transfer and interest payments. **These definitions follow those used elsewhere in the literature ^[Fernandez (2008)],** minimising the effect of methodological differences on this paper's results (Gechert, 2017). We use variables in real terms, while data for GDP was sourced from the ONS already in real terms, the ONS (2023) report that the data used for public sector finances are not adjusted for inflation. Therefore, we process the fiscal variables into real terms, scaling by the deflator rate. ^[Given that the GDP data is in terms of current prices, we also divide by the deflator rate in Q4 2024 (the final quarter in our data). This allows for more intuitive comparisons of the data.] Following Capek and Cuaresma (2020), who highlight that data for estimating fiscal multipliers is typically seasonally adjusted, we applied the X-13 ARIMA-SEATS method to our variables. Gross Domestic Product (GDP) was an exception, as it was sourced after seasonal adjustment. This process reduces noise in the data, leading to more meaningful results. Consistent with Fernández (2006), all variables are log-transformed prior to estimation, except for the short-term interest rate, which enters the model in levels. This will facilitate an intuitive interpretation of results in percentage terms. 
Given that this analysis uses data from 1987, SONIA (which was introduced in 1997) could not be used as the short term interest rate, instead we use the 3 month interbank rates sourced from the Federal Reserve Bank of St Louis. We use the Sterling ERI from the Bank of England, this index is weighted by trade patterns (Lynch and Whitaker, 2005), thus allowing the ERI to effectively capture the effects of the UK's interactions with other economies.


The data series across the sample period are plotted in the data appendix. A key feature of these series is the apparent presence of a deterministic linear trend driving the series upwards (with the exception of the exchange rate and mean short run rate). This suggests that the VAR system may be trend stationary, thus we include a linear deterministic trend in our specification. This will mitigate spurious correlation in the model due to exogenous trending factors.






### Model

```{r Bivariate - Optimal Lags}

clean_data <- na.omit(model_data)

clean_data2 <- na.omit(model_data2) 


tmp <- clean_data[,-c(1,2)]

OptimalLag <-  VARselect(clean_data[,-c(1,2)], lag.max = 5, type = "const")
OptimalLag2 <-  VARselect(clean_data2[,-c(1,2)], lag.max = 5, type = "const")

# OptimalLag2$selection
# OptimalLag$criteria
# OptimalLag$selection
# OptimalLag$criteria
# OptimalLag2$criteria

```







```{r ReducedVAR}
# library(vars)

reduced_VAR <- VAR(clean_data[,-c(1,2)], p = 3, type = "const")
# reduced_VAR2 <- VAR(clean_data2[,-c(1,2)], p = 4, type = "both")
# reduced_VAR <- VAR(clean_data[, -1], p = 4)

# roots(reduced_VAR)
# roots(reduced_VAR2)

# summary(reduced_VAR)

# Summary reports the roots of the polynomial. 


```








## VECM




The reduced-form VAR model can be written as:

\[
X_t = \mu+  A_1 X_{t-1} + \cdots + A_p X_{t-p} + \epsilon_t
\tag{1}
\]

where: \[
X_t = \begin{pmatrix} G_t \\ R_t \\ GDP_t \\ ERI_t \\ \tau_t \\ P_t \end{pmatrix}
\]

Here the vector \(X_t \) defines the endogenous variables, as previously mentioned, we have assumed a lag order  ($p$) of 4. The term \( \mu \) in equation 1 captures the deterministic component of the model. Given the trending behavior between the fiscal variables, the deflator rate, and GDP: we use a linear deterministic trend for this.


Kilian and Lutkepohl (2016) show that the VAR model can be reparameterised as a Vector Error Correction Model (VECM) by subtracting the lagged variables and rearranging terms:  

\[
\Delta X_t = \Pi X_{t-1} + \sum_{i=1}^{k-1} \Gamma_i \Delta X_{t-i} + \mu + \varepsilon_t
\tag{2}
\]

When correctly specified, this representation can yield efficiency gains: including the error correction term may allow the model to capture important dynamics in the long run relationships between variables (Martin, Hurn, and Harris; 2013). We therefore perform tests for cointegration to determine the appropriate specification before estimating the model.

Given the small sample sizes associated with macroeconomic time series, various Monte Carlo simulations have been performed to assess the finite sample properties of cointegration tests. Hubrich, Lutkepohl, and Saikkonen (2001) find that the Likelihood Ratio type tests are among the best performing, with other classes of tests having worse power or size. Therefore we focus on this class of cointegration tests.   Madadala and Kim (1998) provide a review of studies investigating Johansen's cointegration tests, highlighting that: sample sizes upwards of 100 may be needed to appeal to asymptotic results, tests can be misleading when too few variables are included, and insufficient lag lengths can lead to size distortions. Fortunately, these considerations do not appear to be a concern for this research. As previously discussed, due to concerns with underestimating the lag order when evaluating IRFs, we assume a lag order of 4, which we believe to be at least equal to the true lag parameter. 
^[While not relied on for determining the lag order, lag order selection procedures including the Akaike Information Criteria suggest that a lag as low as 2 may be appropriate for the levels VAR. Due to the considerations mentioned, we decide to use a fixed lag order of 4. The sensitivity of the final results to the lag order is considered as a robustness check.] Additionally, we have added additional control variables to the VAR model to mitigate omitted variable bias (Gechert, 2017) which should alleviate the concern raised by Madadala and Kim. Nevertheless, Madadala and Kim highlight further complications of the test. We therefore consider this in detail before testing for cointegration. 


Lutkepohl, Saikkonen, and Trenkler (2001) find that in small samples, while trace tests had size distortions, they had power advantages relative to the maximum eigenvalue test. This suggests that there may be merit in considering both tests. Cheung and Lai (1993), using a Monte Carlo simulation, find that there is finite sample bias in Johansen's cointegration tests, with both the maximimum eigenvalue and trace tests finding cointegration at a rate greater than implied by asymptotic theory. They therefore apply a scaling factor to the asymptotic critical values given by

\[
SF = \frac T {T-kp}
\tag{3}
\]

where $T$ is the number of observations, $k$ the number of variables in the system, and p the lag order. Gonzalo and Lee (1998) provide further evidence of bias in Johansen's cointegration tests, identifying cases where the test detects spurious cointegration. In light of this, we ensure the asymptotic results reported as standard are scaled to account for the finite sample bias in the test.

Aside from the type of cointegration test, Hubrich, Lutkepohl, and Saikkonen (1998) highlight that improperly specified deterministic terms invalidate cointegration testing, leading to reduced power. 
Given that we have suggested the model to follow a deterministic linear trend, we ensure this is captured in our test. Martin, Hurn, and Harris (2013) highlight that a constant deterministic component in cointegration implies a linear trend in the VAR. We therefore include a constant term in cointegration when reporting the following Johansen's tests for cointegration.




**Watson (2000): specification of deterministic components affects the power of tests**

We begin by performing Johansen's maximum eigenvalue test for cointegration, which compares the null hypothesis of rank r, to the alternative of rank r+1. Accounting for the finite sample bias in the test, we find insufficient evidence to reject the null hypothesis of no cointegrating relations at the 5% significance level. Consequently, the VECM reduces to a VAR in the differences of variables (Kilian and Lutkepohl, 2016). Per the previous review of cointegration tests, we also perform Johansen's trace test. This test has as the alternative hypothesis that the rank is greater than assumed under the null hypothesis. Again, we reject the null hypothesis of no cointegration after accounting for the test's finite sample bias. 

Kilian and Lutkepohl (2016) highlight the asymmetric consequences of imposing a unit root. When the underlying data generating process possesses a unit root, the reduced form model can benefit from increased efficiency in estimation by imposing a unit root. Failing to impose a unit root in such a case would only reduce the precision of the Least Squares estimates.  In contrast, when the underlying process does not follow a unit root, incorrectly imposing one would result in over-differencing - yielding an inconsistent estimator. Nevertheless the tests conducted provide little evidence of cointegrating relations, and thus the VAR in differences is viewed as the appropriate specification. As well as the statistical merits of this specification, it yields an intuitive interpretation. As the alternative specification was a VECM with rank 1, we considered the 1st eigenvector for the cointegration relations, and concluded that this did not provide meaningful information on the long run equilibrium behaviour of the system. ^[The reported cointegrating relation was 
 $log\_expenditure = - 0.14 \cdot mean\_SR\_Rate + 1.6  \cdot log\_GDP + 3.9 \cdot log\_ERI -3 \cdot log\_revenue + 9 \cdot log\_deflator + \epsilon_t$  
 where $\epsilon_t$ is I(0). Inflation and the short term rate both represent costs for government spending, so these would be expected to have negative signs. While this was true for the mean short term rate, the cointegrating relation suggested that a shock to the long run equilibrium of the deflator rate led to a large positive increase to expenditure. This is not expected, while it could be argued that the government spending is relatively inelastic, meaning that the government absorbs increased costs rather than reduce spending, the magnitude of the reported effect is difficult to reconcile and the inconsistency in the signs of the 2 variables questions the appropriateness of this cointegrating relation. Additionally, the cointegrating relation suggests that an increase in government spending from the long run equilibrium is associated with a 3 times greater increase in government revenue (all else equal).] Therefore, we proceed in this analysis with the VAR in differences representation of the model, given the differencing, the lag order reduces to 3.   

**For robustness we also perform Phillips-Ouliaris' (1990) residual based test for cointegration. Using this test we again fail to reject the null hypothesis of no cointegration, suggesting that a VECM is not an appropriate representation for our data.** 


 

*Add code results to appendix?*


NB on unit root tests: failing to reject H0 does not mean we accept H0!!!! **(Lutkepohl)**








## Identification


Thus far we have only defined the reduced form VAR. The residuals in this model are not meaningful as they are linearly dependent, instead, the interest of this research lies in the structural model. The structural shocks are mutually uncorrelated and have clear interpretations (Kilian and Lutkepohl, 2016). These shocks will allow us to construct FEVDs and IRFs to report the findings of this paper.

The structural model can be written as 


\[
\beta_0 X_t = \mu+  \beta_1 X_{t-1} + \cdots + \beta_p X_{t-p} + u_t
\tag{4}
\]

Here $u_t$ denotes the structural errors.
We also observe that equation 1 for the reduced form model can be rewritten as:


\[
X_t = \mu+  B_0^{-1}B_1 X_{t-1} + \cdots + B_0^{-1}B_p X_{t-p} + \epsilon_t
\tag{1a}
\]

The term $B_0^{-1}$ is referred to as the structural impact multiplier matrix. Comparing equations 1a and 4, it is clear that knowledge of the structural impact multiplier matrix or its inverse would allow us to move between the structural and reduced form representations, thus recovering the structural shocks. More explicitly, we can write the structural shocks as functions of the reduced form residuals:

\[
\epsilon_t = B u_t 
\tag{5}
\]

As outlined in the literature review, there have been numerous approaches to identifying the structural parameters. This study uses recursive sign restrictions, leveraging the Cholesky decomposition. To recover the structural shocks using this approach, we assume \( B \) to be a lower triangular matrix and that the structural shocks have unitary variance ^[The assumption of unitary variance for the structural shocks is not restrictive as it can be imposed by rescaling the variables. This assumption, however means that the reduced form covariance matrix reduces to equation 6.].  This corresponds to a causal ordering of the transmission of shocks: the first variable is considered the most exogenous, affecting all others. In contrast, the variable ordered last is considered the most endogenous, being contemporaneously effected by shocks to all other variables, whilst having no instantaneous effect itself on the other variables. Having defined this recursive ordering, given that the matrix \( B \) is assumed to be lower triangular, we can estimate it using the Cholesky decomposition of the covariance matrix of the reduced-form residuals (\( \Sigma_\epsilon \)):

\[ 
\Sigma_\epsilon = E[\epsilon_t \epsilon_t'] = BB'
\tag{6}
\]

Therefore we have that $B = Chol(\Sigma_\epsilon)$

This paper uses the following recursive ordering:

\[
(G, R, GDP, ERI, T, P) 
\tag{7}
\]  

With the variables corresponding to government expenditure, the short term interest rate, GDP, the Exchange Rate Index, net taxes, and the GDP deflator respectively.

The matrix \( B \) has the form:

\[
B = \begin{pmatrix} 
b_{11} & 0 & 0 & 0 & 0 &0 \\ 
b_{21} & b_{22} & 0 & 0 & 0 &0 \\ 
b_{31} & b_{32} & b_{33} & 0 & 0 &0 \\ 
b_{41} & b_{42} & b_{43} & b_{44} & 0 &0 \\ 
b_{51} & b_{52} & b_{53} & b_{54} & b_{55} &0 \\
b_{61} & b_{62} & b_{63} & b_{64} & b_{65} & b_{65} 
\end{pmatrix}
\tag{8}
\]

Thus, the structural shocks \( u_t \) can be recovered as:

\[ 
u_t = B^{-1} \epsilon_t 
\tag{5a}
\]




Killian and Lutkepohl (2017) highlight that identification of the structural parameters is not a purely statistical concern, the restrictions must also be economically meaningful for the resulting structural parameters to be identified. Therefore we proceed with an exposition of the economic assumptions implicit in our identifying restrictions imposed by the matrix $B$. The following is comparable to Fernandez's (2006) identifying assumptions.  

Blanchard and Perotti (2002) argue that the use of quarterly data allows government spending to be treated as predetermined with respect to the rest of the variables within the quarter. This is motivated by implementation lags for changes to government spending and consequently this is ordered first. Given physical constraints, the interest rate is assumed not to react contemporaneously to price, net taxes, output, or the exchange rate. Thus the short term rate is considered the next most exogenous variable. However monetary policy shocks are assumed to affect output, net taxes, prices, and the exchange rate contemporaneously. Fernandez (2006) justifies this assumption by noting that interest movements are 
anticipated and thus they can be transmitted to real variables relatively quickly. Shocks to the exchange rate are assumed to affect net taxes contemporaneously as households adjust to changes in the cost of imports. Investment plans take time to adjust, as does consumption due to internal habit which leads to highly persistent levels of consumption. Consequently, we do not expect shocks to net taxes to affect activity. However, shocks to economic activity is expected to affect contemporaneous tax receipts to the government as households respond to prevailing economic conditions. Despite the aforementioned internal habit, this behaviour is expected as households attempt to smooth fluctuations in their consumption path, consequently they may adjust their behaviours towards consumption and saving, which is expected to have tax implications. Due to price stickiness, prices do not react contemporaneously to shocks to GDP. 

# Results

Given that we have performed cointegration tests, it would suggest that the appropriate model has been chosen, and thus we would expect the stability of the VAR model. This is supported by inspecting the eigenvalues of the VAR model, the greatest of which is `r round(max(roots(reduced_VAR)),2)` suggesting that we do not have issues of explosive roots. We therefore proceed by presenting the results of the SVAR analysis, focusing on Forecast Error Variance Decompositions (FEVDs) and Impulse Response Functions (IRFs)




```{r structural_VAR}

# Define the 5 dimensional lower triangular matrix, A

# Recover structural VAR using Cholesky decomposition

# Amat <- matrix(c(1, 0, 0, 0, 0,   # Recursive ordering
#                  NA, 1, 0, 0, 0,  
#                  NA, NA, 1, 0, 0,  
#                  NA, NA, NA, 1, 0,  
#                  NA, NA, NA, NA, 1), 
#                nrow = 5, byrow = TRUE)


# Variables are already ordered per the recursive identification strategy. Thus create a lower triangular matrix 
k <- ncol(clean_data[,-c(1,2)])
Amat <- diag(1, k)
Amat[lower.tri(Amat)] <- NA
# print(Amat)




svar_model <- SVAR(reduced_VAR, Amat = Amat, estmethod = "direct")
# svar_model2 <- SVAR(reduced_VAR2, Amat = Amat, estmethod = "direct")
# ?SVAR()

structural_shocks <- residuals(svar_model)

# svar_model


irf_result <- irf(svar_model, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = FALSE)  # Forecast horizons
# irf_result2 <- irf(svar_model2, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = FALSE)  # Forecast horizons
# irf_result <- irf(svar_model, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = TRUE)  # Forecast horizons
# plot(irf_result)
# Visualize IRFs

FEVD_result <- fevd(svar_model, n.ahead = 10)  # Forecast horizons
# FEVD_result2 <- fevd(svar_model2, n.ahead = 10)  # Forecast horizons

# plot(FEVD_result)
# plot(FEVD_result2)



# Post Covid:

```


## IRFs

```{r IRFs_Table}




# Extract IRFs
exp_irf <- round(irf_result$irf$dif_log_expenditure[, 3],2)
rev_irf <- round(irf_result$irf$dif_log_revenue[, 3],2)


exp_irf_df <- as.data.frame(
  cbind(
    seq(0,10),
    irf_result$irf$dif_log_expenditure[, 3],
    irf_result$Lower$dif_log_expenditure[, 3],
    irf_result$Upper$dif_log_expenditure[, 3]
  )
)

colnames(exp_irf_df) <- c("Quarter","Expenditure", "LB", "UB")

rev_irf_df <- as.data.frame(
  cbind(
    seq(0,10),
    irf_result$irf$dif_log_revenue[, 3],
    irf_result$Lower$dif_log_revenue[, 3],
    irf_result$Upper$dif_log_revenue[, 3]
  )
)

colnames(rev_irf_df) <- c("Quarter","Revenue", "LB", "UB")









```


```{r Restricted-Model-Expenditure-IRF, fig.cap="Impulse Response Function of GDP Following a Shock to Expenditure ", fig.align='center'}

# Define a custom theme for consistency
custom_theme <- theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# Plot for Expenditure IRF
ggplot(exp_irf_df, aes(x = Quarter)) +
  geom_ribbon(aes(ymin = LB, ymax = UB), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Expenditure), color = "blue", size = 1.2) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  labs(
    title = "Impulse Response Function",
    x = "Quarter",
    y = "Log-Difference Response"
  ) +
  custom_theme

```


```{r Restricted-Model-Revenue-IRF, fig.cap="Impulse Response Function of GDP Following a Shock to Revenue ", fig.align='center'}
# Plot for Revenue IRF
ggplot(rev_irf_df, aes(x = Quarter)) +
  geom_ribbon(aes(ymin = LB, ymax = UB), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Revenue), color = "blue", size = 1.2) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  labs(
    title = "Impulse Response Function",
    x = "Quarter",
    y = "Log-Difference Response"
  ) +
  custom_theme
```



We report the cumulative structural impulse response functions up to a horizon of 10 quarters including bands for the 68% confidence intervals. Kilian and Lutkepohl (2016) highlight that given the short sample sizes VAR models are typically estimated on, this level is more appropriate than the typical 5% significance level. Furthermore, given the low sample size we do not appeal to asymptotic results, instead we report a bootstrap confidence interval. Using a 68% confidence interval therefore requires fewer iterations to accurate estimate this interval. The IRF function from the VARS library in R implements Efron's (1992) percentile interval. It should be highlighted that unlike standard confidence intervals constructed using standard errors, Efron's percentile interval are often asymmetric about the point estimate (Efron, 1981). Regarding the maximum forecast horizon of 10 quarters, it is believed that the majority of the response of GDP to the fiscal shocks should be captured within this window, additionally, extrapolating further out, we would have less confidence in the point estimates.  

Here we focus on the response of GDP to the fiscal variables, figure 1 summarises the results for a transitory shock to government expenditure. Given the model specification, the results are interpreted as elasticities in terms of the percentage growth of the variables. We find a point estimate of `r round(irf_result$irf$dif_log_expenditure[1, 3],2)` in the period of the shock, suggesting that a standard deviation shock to the percentage growth of expenditure is associated with a reduction in GDP growth of `r cat(round(irf_result$irf$dif_log_expenditure[1, 3],2) * 100, "%")` **(-17% of a standard deviation?)**. The confidence interval for this IRF is `r  cat("(",round(irf_result$Lower$dif_log_expenditure[1, 3],2), ",", round(irf_result$Upper$dif_log_expenditure[1, 3],2), ")")` - ( -0.29 , -0.08 ). As 0 is not contained in this interval, it suggests that there is a statistically significant crowding out effect of government expenditure. Nevertheless, tracing the IRF out over time, by the next quarter the cumulative response is marginally positive (`r round(irf_result$irf$dif_log_expenditure[2, 3],2)`). This reversal is only temporary, with the cumulative IRF returning to a negative value the next quarter, then stabilising at a value of `r round(irf_result$irf$dif_log_expenditure[11, 3],2)` by the 10th quarter. However, the percentile intervals for these later IRFs contain 0 and thus these results are not statistically significant. While we do not discuss in detail the interpretation of these results in this section, this suggests a weak crowding out effect of government spending.


Considering now the response of GDP to a shock to government revenues, we find no effect in the quarter of the shock. However, the following 4 quarters see positive responses, with the cumulative IRF peaking a year after the initial shock and remaining within this region. Based on the bootstrap confidence interval, this effect is statistically significant. Additionally, the response of a positive shock to government revenue is itself positive, conflicting with the results from the literature. We analyse this in our discussion and provide a hypothesis for this result, as well as its implications for policy. 

```{r, Multipliers}
# Constructing cumulative present value multipliers

disc_rate <- mean(data$mean_SR_Rate)
horizon <- length(exp_irf)
discount_factors <- 1 / ((1 + disc_rate)^(0:(horizon - 1)))


level_irf_exp_response <- exp(exp_irf) - 1  # % change in Y relative to baseline
level_irf_rev_response <- exp(rev_irf) - 1  # % change in G relative to baseline
level_irf_rev <- exp(round(irf_result$irf$dif_log_revenue[, 1],2)) - 1  # % change in G relative to baseline

level_irf_exp <- exp(round(irf_result$irf$dif_log_expenditure[, 5],2)) - 1  # % change in G relative to baseline

mean_gdp <-  mean(data$GDP)
mean_expenditure <-  mean(data$Expenditure_SA)
mean_revenue <-  mean(data$Revenue_SA)

pv_exp_response <- sum(level_irf_exp_response * discount_factors)
pv_exp <- 1 + sum(level_irf_exp * discount_factors)
Exp_PV = pv_exp_response/pv_exp
exp_multiplier <- Exp_PV *(mean_gdp/mean_expenditure)
# Exp_PV



pv_rev_response <- sum(level_irf_rev_response * discount_factors)
pv_rev <- 1 + sum(level_irf_rev * discount_factors)
Rev_PV = pv_rev_response/pv_rev
rev_multiplier <- Rev_PV *(mean_gdp/mean_revenue)

# Rev_PV


```

Following Mountford and Uhlig (2009) we define our present value multipliers at the 10 quarter horizon as:

\[
\text{Fiscal Multiplier}_H = 
\frac{
\sum_{h=0}^{H} (1 + r)^h \cdot y_h
}{
\sum_{h=0}^{H} (1 + r)^h \cdot f_h
}
\cdot
\frac{\bar{y}}{\bar{f}}
\tag{9}
\]



Where $y_h$ denotes the response of GDP to the fiscal instrument at horizon h ^[Given the VAR in differences specification of this paper, the terms $y_h$ and $f_h$ are not the IRF estimates. We must recover these by taking the exponent of our estimates and subtracting 1. The differences of logarithms is equal to the logarithm of the ratio, thus through exponentiation we obtain the ratio. Subtracting 1 from this yields the response of the variable, as desired.], $f_h$ the response of the fiscal instrument to itself at horizon h, H the total horizon (10 quarters in this analysis), and $r$ the mean short term interest rate (which we calculate to be `r round(disc_rate,2)` to 2 basis points). The term $\frac{\bar{y}}{\bar{f}}$ is a scaling factor representing the average ratio of GDP to the fiscal instrument, given that the IRFs are in terms of the fiscal instrument, this allows for an interpretation in terms of the share of GDP. Ramey (2019) argues that using this ratio can make multiplies appear more countercyclical, instead they propose that the modeled variables are divided by lagged GDP to mitigate this. However, Owyang, Ramey, Zubairy note that this transformation is difficult to perform in the typical VAR methods for computings IRFs. They instead rely on Jorda's (2005) local projections estimator of the IRF. Kilian and Lutkepohl (2016) provide an account of the limitations of this framework, therefore we apply the scaling factor. While this would subject our multipliers to Ramey's (2019) critique, it allows for a better comparison with the literature and avoids the possible incorrect implementation of the proposed adjustment. 

Performing these calculations, we find a multiplier for government expenditure of `r round(exp_multiplier, 2)` and of `r round(rev_multiplier, 2)` for government revenue. Both are less than 1 in magnitude, suggesting relatively small multipliers. In the next section we consider the robustness of these multipliers to misspecifications in our model, we then discuss the implications of these results.
 



```{r, IRFs1}



# structural_shocks <- residuals(svar_model)
# irf_result <- irf(svar_model, n.ahead = 10)  # Forecast horizons
# plot(irf_result)  # Visualize IRFs


```



# Robustness


```{r, Restricted Model Diagnostics}

# normality.test(reduced_VAR)

serial_test <- serial.test(reduced_VAR, lags.pt = 16, type = "PT.asymptotic")


# serial_test # Don't reject H0 of no autocorrelation

normality_test <- normality.test(reduced_VAR)
# normality_test


hetero_test <- arch.test(reduced_VAR, lags.multi = 5)
# hetero_test

```

## Chow test for Structural Breaks


```{r, Lutkepohl and Candelon (2000) Residual-based Bootstrap Chow Test}

# --- Step 1: Define Restricted Model ---
# reduced_VAR <- VAR(clean_data[,-c(1,2)], p = 3, type = "const")
# --- Step 1: Define Parameters ---
break_year <- 2008
p <- 1
Rep <- 1000
T_break <- which((clean_data$Year == break_year))[1]  # First observation of break year


# --- Step 2: Estimate VAR & Prepare Data ---
Y <- clean_data[, -c(1, 2)]  # Drop Year + Quarter columns
var_orig <- VAR(Y, p = p, type = "const")
resids <- residuals(var_orig)
centered_resids <- scale(resids, scale = FALSE)  # Centered residuals
Y_vals <- as.matrix(Y)
presample <- Y_vals[1:p, ]
coef_matrix <- Bcoef(var_orig)
A <- as.matrix(coef_matrix[, -ncol(coef_matrix)])
c <- coef_matrix[, ncol(coef_matrix)]

# --- Step 3: Define Chow Function (Lambda_BP) ---
chow_stat <- function(Y, T_break, p) {
  tryCatch({
    
    T_full  <- nrow(Y) 
    
    # Restricted model: full sample
    model_restricted <- VAR(Y, p = p, type = "const")
    SSR_restricted <- sum(residuals(model_restricted)^2)
    
    # Unrestricted model: separate pre and post samples
    Y_pre <- Y[1:T_break, ]
    Y_post <- Y[(T_break + 1):T_full, ]
    
    model_pre <- VAR(Y_pre, p = p, type = "const")
    model_post <- VAR(Y_post, p = p, type = "const")
    
    SSR_unrestricted <- sum(residuals(model_pre)^2) + sum(residuals(model_post)^2)
    SSR_1 <- sum(residuals(model_pre)^2)
    SSR_tot <- sum(residuals(model_restricted)^2)


    # Extract log-likelihoods
    ll_restricted <- logLik(model_restricted)
    ll_unrestricted <- logLik(model_pre) + logLik(model_post)
    
    
    
    # Count number of coefficients per equation (excluding stats like SE, t)
    k <- nrow(coef(reduced_VAR)[[1]])
    n_eq <- length(coef(reduced_VAR))  # Number of equations
    
    # Degrees of freedom = number of extra parameters in unrestricted models
    df_chow <<- k * n_eq


    # Compute LR test statistic
    lambda_BP <- -2 * (as.numeric(ll_restricted) - as.numeric(ll_unrestricted))
 
    return(lambda_BP)
  }, error = function(e) {
    message("Chow statistic failed: ", e$message)
    return(NA)
  })
}

# --- Step 4: Bootstrap Simulation ---
set.seed(1234)
boot_stats <- numeric(Rep)

for (r in 1:Rep) {
  # FIX: Sample from available residuals only
  u_star <- centered_resids[sample(1:nrow(centered_resids), size = nrow(Y_vals), replace = TRUE), ]

  Y_boot <- matrix(NA, nrow = nrow(Y_vals), ncol = ncol(Y_vals),
                   dimnames = list(NULL, colnames(Y_vals)))
  Y_boot[1:p, ] <- presample

  for (t in (p + 1):nrow(Y_vals)) {
    Y_lags <- as.vector(t(Y_boot[(t - p):(t - 1), ]))
    Y_boot[t, ] <- c + A %*% Y_lags + u_star[t, ]
  }

  boot_stats[r] <- if (anyNA(Y_boot)) NA else chow_stat(Y_boot, T_break, p)
  # if (r %% 50 == 0) cat("Completed iteration", r, "\n")
}

boot_stats <- na.omit(boot_stats)

# --- Step 5: Actual Statistic & p-value ---
actual_stat <- chow_stat(Y_vals, T_break, p)
p_value_asymptotic <- 1 - pchisq(actual_stat, df_chow)
p_val <- mean(boot_stats > actual_stat)

# --- Step 6: Plot Results ---
if (length(boot_stats) > 0) {
  boot_df <- data.frame(
    Iteration = 1:length(boot_stats),
    Chow_Statistic = boot_stats
  )

 chow_plot <<-  ggplot(boot_df, aes(x = Iteration, y = Chow_Statistic)) +
    geom_line(color = "dodgerblue", alpha = 0.7) +
    geom_hline(yintercept = actual_stat, color = "red", linetype = "dashed", linewidth = 1) +
    geom_point(aes(x = 0, y = actual_stat), color = "red", size = 3) +
    labs(
      title = "Bootstrap Chow Test (Recursive)",
      subtitle = paste("Breakpoint:", break_year, "| p-value:", round(p_val, 4)),
      x = "Bootstrap Iteration",
      y = "Chow Test Statistic"
    ) +
    theme_minimal()
} else {
  warning("No valid bootstrap statistics available to plot.")
}

# print(actual_stat)
# print(p_val)
# print(p_value_asymptotic)


```

Kilian and Lutkepohl (2016) highlight parameter instability as a major concern, noting that this would violate the assumption of stationary needed to motivate VAR analysis. Structural change is a common example of such instability, a posible candidate for this being the GFC, with Ramey (2019) noting that "Fiscal multipliers might be different in the wake of a financial crisis". We therefore consider as a test for robustness whether the parameters have remained stable across the period, using 2008 as a potential breakpoint.

Using an asymptotic likelihood based Chow test we find significant evidence to reject the null hypothesis of no parameter instability. Nevertheless, Lutkepohl and Candelon (2000) highlight that when the sample size is low relative to the nuber of parameters, the Chow test may have distorted size. Given that we are considering a 6-dimensional VAR in this analysis, with a sample period of less than 40 yeas at a quarterly frequency, this issue is likely present in this analysis. Therefore we also consider a bootstrap version of the Chow statistic the authors show to have more desirable finite sample properties. Following Lutkepohl and Candelon (2000), we implement our recursive residual-based boostrap for the Chow test as follows:
\begin{enumerate}
  \item Estimate the restricted VAR model (pooling both periods around the GFC)
  \item Obtain the demeaned residuals from the restricted model.  
  \item Generate bootstrap residuals by randomly drawing from the centered residuals with replacement.
  \item For each iteration, estimate the restricted and unrestricted models (where we allow the parameters to vary across the samples).
  \item Compute the Chow statistic (Here we use the same likelihood based Chow test as with our asymptotic testing).
\end{enumerate}

```{r chow-plot, fig.cap="Bootstrap Chow Test Results", fig.align='center'}
chow_plot+ 
  custom_theme
```

Through this process we can derive the empirical distribution of the bootstrap time series. We then compute the pseudo p-value as the percentage of times the bootstrap statistic exceeds the test statistic. Using `r Rep` replications, we find a p-value of 0, this can be further seen in figure 3 where the test statistic exceeds even the greatest critcal value from the bootstrap distribution. 
This supports the previous asymptotic test of parameter instability. We therefore estimate the SVAR on the post-GFC sample to determine whether there are differences in the estimated fiscal multipliers in the most recent period. 





### Post GFC Shocks

```{r, SVAR 2}

T_break <- which((clean_data$Year == break_year))[1]  # First observation of break year

Y_post_GFC <-  Y[(T_break + 1):nrow(Y), ]

# chow_check <- clean_data[(T_break + 1):nrow(Y), ]

# reduced_VAR <- VAR(clean_data[,-c(1,2)], p = 3, type = "const")


reduced_VAR_post_GFC <- VAR(Y_post_GFC, p = 3, type = "const")

svar_model_post_gfc <- SVAR(reduced_VAR_post_GFC, Amat = Amat, estmethod = "direct")

irf_result_post_gfc <- irf(svar_model_post_gfc, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = FALSE)
```

```{r IRFs_Table Post GFC}

exp_irf_post_GFC <- round(irf_result_post_gfc$irf$dif_log_expenditure[, 3],2)
rev_irf_post_GFC <- round(irf_result_post_gfc$irf$dif_log_revenue[, 3],2)

# Combine into one table
# Combine IRFs into one table
irf_combined <- cbind(
  Period = 0:(length(exp_irf) - 1),
  Total_Expenditure = exp_irf,
  Total_Revenue = rev_irf,
  PostGFC_Expenditure = exp_irf_post_GFC,
  PostGFC_Revenue = rev_irf_post_GFC
)

# Prepare data for Revenue plot
df_rev <- data.frame(
  Period = 0:(length(rev_irf) - 1),
  Total = rev_irf,
  PostGFC = rev_irf_post_GFC
) %>%
  pivot_longer(cols = -Period, names_to = "Sample", values_to = "IRF")


# Prepare data for Expenditure plot
df_exp <- data.frame(
  Period = 0:(length(exp_irf) - 1),
  Total = exp_irf,
  PostGFC = exp_irf_post_GFC
) %>%
  pivot_longer(cols = -Period, names_to = "Sample", values_to = "IRF")


```

```{r, Multipliers Post GFC}
# Constructing cumulative present value multipliers




disc_rate_post_gfc <- mean(data[(T_break + 1):nrow(data), ]$mean_SR_Rate)
# horizon <- length(exp_irf)
discount_factors_post_gfc <- 1 / ((1 + disc_rate_post_gfc)^(0:(horizon - 1)))


level_irf_exp_response_post_gfc <- exp(exp_irf_post_GFC) - 1  # % change in Y relative to baseline
level_irf_rev_response_post_gfc <- exp(rev_irf_post_GFC) - 1  # % change in G relative to baseline
level_irf_rev_post_GFC <- exp(round(irf_result_post_gfc$irf$dif_log_revenue[, 1],2)) - 1  # % change in G relative to baseline

level_irf_exp <- exp(round(irf_result_post_gfc$irf$dif_log_expenditure[, 5],2)) - 1  # % change in G relative to baseline

mean_gdp_post_gfc <-  mean(data[(T_break + 1):nrow(data), ]$GDP)
mean_expenditure_post_gfc <-  mean(data[(T_break + 1):nrow(data), ]$Expenditure_SA)
mean_revenue_post_gfc <-  mean(data[(T_break + 1):nrow(data), ]$Revenue_SA)

pv_exp_response_post_gfc <- sum(level_irf_exp_response_post_gfc * discount_factors_post_gfc)
pv_exp_post_gfc <- 1 + sum(level_irf_rev_post_GFC * discount_factors_post_gfc)
Exp_PV_post_gfc = pv_exp_response_post_gfc/pv_exp_post_gfc
exp_multiplier_post_gfc <- Exp_PV_post_gfc *(mean_gdp/mean_expenditure_post_gfc)
# Exp_PV



pv_rev_response_post_gfc <- sum(level_irf_rev_response_post_gfc * discount_factors_post_gfc)
pv_rev_post_gfc <- 1 + sum(level_irf_rev_post_GFC * discount_factors_post_gfc)
Rev_PV_post_gfc = pv_rev_response_post_gfc/pv_rev_post_gfc
rev_multiplier_post_gfc <- Rev_PV_post_gfc *(mean_gdp_post_gfc/mean_revenue_post_gfc)



# Accounting for the effect of the low interest rate environment since the GFC

pv_exp_response_post_gfc_rate_2 <- sum(level_irf_exp_response_post_gfc * discount_factors)
pv_exp_post_gfc_rate_2 <- 1 + sum(level_irf_rev_post_GFC * discount_factors)
Exp_PV_post_gfc_rate_2 = pv_exp_response_post_gfc_rate_2/pv_exp_post_gfc_rate_2


exp_multiplier_post_gfc_rate2 <- Exp_PV_post_gfc_rate_2 *(mean_gdp_post_gfc/mean_revenue_post_gfc)

  
pv_rev_response_post_gfc_rate2 <- sum(level_irf_rev_response_post_gfc * discount_factors)
pv_rev_post_gfc_rate2 <- 1 + sum(level_irf_rev_post_GFC * discount_factors)
Rev_PV_post_gfc_rate2 = pv_rev_response_post_gfc_rate2/pv_rev_post_gfc_rate2
  
rev_multiplier_post_gfc_rate2 <- Rev_PV_post_gfc_rate2 *(mean_gdp_post_gfc/mean_revenue_post_gfc)

# exp_multiplier_post_gfc_rate2
# rev_multiplier_post_gfc_rate2
# Rev_PV
```


```{r IRF-comp-Expenditure, fig.cap="GDP Responses to Government Compared", fig.align='center'}
# Plot Expenditure IRFs
ggplot(df_exp, aes(x = Period, y = IRF, color = Sample)) +
  geom_line(size = 1.2) +
  labs(
    title = "IRFs Over Time (Restricted vs Unrestricted Model)",
    x = "Quarters Ahead",
    y = "Impulse Response"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  custom_theme
```


```{r IRF-comp-Revenue, fig.cap="GDP Responses to Government Revenue Compared", fig.align='center'}
# Plot Revenue IRFs
ggplot(df_rev, aes(x = Period, y = IRF, color = Sample)) +
  geom_line(size = 1.2) +
  labs(
    title = "IRFs Over Time (Restricted vs Unrestricted Model)",
    x = "Quarters Ahead",
    y = "Impulse Response"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  custom_theme
```



Figures 3 and 4 compare the IRFs under the restricted model and unrestricted model (post GFC). Only the point estimates are reported, given the reduced sample size while keeping the number of model parameters fixed, we expect greater uncertainty in the IRF estimates, reflected in the confidence intervals. ^[It should be noted that the confidence intervals under this specification show noticeable asymmetry. Efron (1981) highlights that this is expected in small samples. This thus supports the use of the restricted model in order to obtain more meaningful estimates, which we can test for significance.] 

For expenditure, the post GFC IRF follows a similar path to the total period, despite overall reporting slightly lower point estimates. Nevertheless, the results for the unrestricted model are contained within the confidence interval for reported for the overall period. Thus this supports the low responsiveness of GDP to shocks to government spending. 

The IRF for GDP following a shock to government revenue however are a much greater magnitude and remain positive. The IRF peaks at 1.5, again a year after the initial shock. This result suggests that the positive elasticity of GDP growth to shocks to growth in government revenue have been driven by the post GFC period. 

 
 
 
## Lag length

When defining the empirical specification, we highlighted that the lag order was chosen a priori, and that information criteria suggested a lower order lag was compatible with the data. Therefore, here we estimate the model on a single lag (consistent with 2 lags in the levels VAR). Given the evidence of parameter instability, this is reported for both the full sample and the post GFC data. Figures 5 and 6 report the IRFs for government expenditure and revenue respectively across all of the specifications used for robustness testing.


```{r, SVAR reduced lag length}

# T_break <- which((clean_data$Year == break_year))[1]  # First observation of break year
# 
# Y_post_GFC <-  Y[(T_break + 1):nrow(Y), ]


# reduced_VAR <- VAR(clean_data[,-c(1,2)], p = 3, type = "const")


reduced_VAR_1_lag <- VAR(clean_data[,-c(1,2)], p = 1, type = "const")


svar_model_1_lag <- SVAR(reduced_VAR_1_lag, Amat = Amat, estmethod = "direct")

irf_result_1_lag <- irf(svar_model_1_lag, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = FALSE) 


exp_irf_1_lag <- round(irf_result_1_lag$irf$dif_log_expenditure[, 3],2)
rev_irf_1_lag <- round(irf_result_1_lag$irf$dif_log_revenue[, 3],2)


# Prepare data for Revenue plot
df_rev_all <- data.frame(
  Period = 0:(length(rev_irf) - 1),
  Baseline_lags = rev_irf,
  Reduced_lags = rev_irf_1_lag

  
) %>%
  pivot_longer(cols = -Period, names_to = "Sample", values_to = "IRF")


# Prepare data for Expenditure plot
df_exp_all <- data.frame(
  Period = 0:(length(exp_irf) - 1),
  Baseline_lags = exp_irf,
  Reduced_lags = exp_irf_1_lag

) %>%
  pivot_longer(cols = -Period, names_to = "Sample", values_to = "IRF")


```


```{r IRF-comp-Revenue_all, fig.cap="GDP Responses to Government Revenue Compared 2", fig.align='center'}
# Plot Revenue IRFs
ggplot(df_rev_all, aes(x = Period, y = IRF, color = Sample)) +
  geom_line(size = 1.2) +
  labs(
    title = "IRFs Over Time (Restricted vs Unrestricted Model)",
    x = "Periods Ahead",
    y = "Impulse Response"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  custom_theme
```

```{r IRF-comp-Expenditure_all, fig.cap="GDP Responses to Government Compared 2", fig.align='center'}
# Plot Expenditure IRFs
ggplot(df_exp_all, aes(x = Period, y = IRF, color = Sample)) +
  geom_line(size = 1.2) +
  labs(
    title = "IRF: Expenditure Response to Expenditure Shock",
    x = "Periods Ahead",
    y = "Impulse Response"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) + 
  custom_theme
```



```{r, Multipliers 1 lag}
# Constructing cumulative present value multipliers

level_irf_exp_response_1_lag <- exp(exp_irf_1_lag) - 1  # % change in Y relative to baseline
level_irf_rev_response_1_lag <- exp(rev_irf_1_lag) - 1  # % change in G relative to baseline
level_irf_rev_1_lag <- exp(round(irf_result_1_lag$irf$dif_log_revenue[, 1],2)) - 1  # % change in G relative to baseline

level_irf_exp_1_lag <- exp(round(irf_result_1_lag$irf$dif_log_expenditure[, 5],2)) - 1  # % change in G relative to baseline


pv_exp_response_1_lag <- sum(level_irf_exp_response_1_lag * discount_factors)
pv_exp_1_lag <- 1 + sum(level_irf_exp_1_lag * discount_factors)
Exp_PV_1_lag = pv_exp_response_1_lag/pv_exp_1_lag
exp_multiplier_1_lag <- Exp_PV_1_lag *(mean_gdp/mean_expenditure)



pv_rev_response_1_lag <- sum(level_irf_rev_response_1_lag * discount_factors)
pv_rev_1_lag <- 1 + sum(level_irf_rev_1_lag * discount_factors)
Rev_PV_1_lag = pv_rev_response_1_lag/pv_rev_1_lag
rev_multiplier_1_lag <- Rev_PV_1_lag *(mean_gdp/mean_revenue)

# exp_multiplier_1_lag
# rev_multiplier_1_lag


```



consider reduced lag length (improved efficiency/ parsimony)


# Discussion/ Policy Implications


```{r FEVD Table Output}

# Create a data frame with separate columns
multiplier_table <- data.frame(
  Specification = c("1 lag", "Post-GFC", "Baseline"),
  Expenditure = c(exp_multiplier_1_lag, exp_multiplier_post_gfc, exp_multiplier),
  Revenue     = c(rev_multiplier_1_lag, rev_multiplier_post_gfc, rev_multiplier)
)

# Reshape to long format for kable
long_table <- pivot_longer(multiplier_table, cols = c(Expenditure, Revenue),
                           names_to = "Fiscal Instrument", values_to = "Multiplier")%>%
  mutate(Multiplier = round(Multiplier, 2))  %>%
  arrange(`Fiscal Instrument`, Specification)



# Display the table
# kable(long_table, caption = "Present Value Multipliers by Specification and Fiscal Instrument")

```


**Interpret results**
**Compare with lit**
**Policy Implications**
**Future Reserach**
Table 1 summarises the results of this paper. Overall we find weak evidence for a crowding out effect of governement spending. This response is both 

`r kable(long_table, caption = "Present Value Multipliers by Specification and Fiscal Instrument")`


This analysis has found fiscal multipliers that are practically and statistically insignificant

suggests that there is scope to reduce the government deficit through more aggressive fiscal consolidation measures and thus mitigate against the growing costs of the large deficit. 

Results for revenue, baseline model suggests that positive shocks to government revenue growth lead to increased GDP growth. This result is also statistically significant. We note, however, that this result is not robust to our specification checks. Nevertheless, we discuss the implications of such a finding if true.

Fernandez (2007) finds a positive response of GDP to a shock to net taxes, however they attribute this to an increase in government responding. Given the low elasticity of GDP in response to government spending, this explanation would not apply... 

This result is counter intuitive: the primary components of the government revenue variable used by this analysis are taxes, which would be expected to decrease output. Nevertheless, we consider some explanations.

This analysis has considered the government's revenue as opposed to the tax rate, it is possible that the results of a positive multiplier for government revenue are showing the effects of the Laffer curve: by reducing tax rates economic activity is encouraged in turn leading to greater tax receipts.

Alternatively, this paper has used a linear VAR model, meaning that results are symmetric. Consequently, the baseline result of the output multiplier for government revenue suggests that a negative shock to government revenue leads to a reduction in output. This could be the case due to the decreased competitiveness of UK tax rates, meaning that FDI is discouraged ... 

This also highlights limitations of previous SVAR analyses of fiscal multipliers. B+P, ... use data net taxes which the government does not have full control over. Instead the government controls tax rates and tax legislation. While these would be expected to be positively correlated with government revenue this is not necessarily the case.

Nguyem, Onnis, and Rossi (2020) estimate the effects of income and consumption tax changes, 

cf with research and theory

Lucas critique/ extrapolating.


## Limitations
- potential heterogeneity within expedniture/ revenue
- debt sustainability (effects of policy on inflation and interest rate, changing real cost)


# Conclusion

This paper has employed a SVAR analysis of fiscal multipliers using a 6-dimensional VAR with quarterly data. We use recursive short run restrictions leveraging economic theory to determine the transmission of shocks across the VAR model thus identifying the structural parameters. We find a negligible and insignificant multiplier effect of government expenditure. For government revenue we report a positive multiplier with a 68% bootstrap confidence interval excluding 0. We perform tests for misspecification to assess the robustness of these results. Across these specifications we find similar, negligible results for the multiplier of government spending. We show, however, that the IRF for government revenue varies across these alternative specifications. We discuss the policy implications of these results. The low reported multipliers for government expenditure suggest scope for the UK government to reduce its spending to improve its fiscal position, without counterproductive effects through a hysterisis effect. 

Summary of methodology, this research's results, contributions, and future implications.


# Bibliography

Alesina, A. and Ardagna, S., 1998. Tales of fiscal adjustment. Economic policy, 13(27), pp.488-545.

Cheung, Y.W. and Lai, K.S., 1993. FINITE-SAMPLE SIZES OF JOHANSEN'S LIKELIHOOD RATIC) TESTS FOR. Oxford Bulletin of Economics and statistics, 55(3), p.1025.



Efron, B., 1981. Nonparametric standard errors and confidence intervals. canadian Journal of Statistics, 9(2), pp.139-158.

Efron, B., 1992. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution (pp. 569-593). New York, NY: Springer New York.

Efron, B. and Tibshirani, R., 1986. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical science, pp.54-75.

Lüutkepohl and Candelon (2000) https://www.econstor.eu/bitstream/10419/62173/1/723877181.pdf

Lüutkepohl, H., Saikkonen, P. and Trenkler, C., 2001. Maximum eigenvalue versus trace tests for the cointegrating rank of a VAR process. The Econometrics Journal, 4(2), pp.287-310.

Lynch, B. and Whitaker, S., 2004. The new sterling ERI. Bank of England Quarterly Bulletin, Winter.
Maddala, G.S. and Kim, I.M., 1998. Unit roots, cointegration, and structural change.

Martin, V., Hurn, S. and Harris, D., 2013. Econometric modelling with time series: specification, estimation and testing. Cambridge University Press.

European system of accounts - ESA 2010 (2013). Available at: https://ec.europa.eu/eurostat/documents/3859598/5925693/KS-02-13-269-EN.PDF.pdf/44cd9d01-bc64-40e5-bd40-d17df0c69334?t=1414781932000 (Accessed: 11 August 2025). 

Office for National Statistics (2023) Public sector finances QMI, Public sector finances QMI - Office for National Statistics. Available at: https://www.ons.gov.uk/economy/governmentpublicsectorandtaxes/publicsectorfinance/methodologies/publicsectorfinancesandgovernmentdeficitanddebtunderthemaastrichttreatyqmi (Accessed: 12 August 2025). 

UK government debt and deficit: December 2023 (2024) UK government debt and deficit - Office for National Statistics. Available at: https://www.ons.gov.uk/economy/governmentpublicsectorandtaxes/publicspending/bulletins/ukgovernmentdebtanddeficitforeurostatmaast/december2023 (Accessed: 11 August 2025). 

# Data Appendix

## Data  Sources
- Fiscal Variables (not seasonally adjusted):  
https://www.ons.gov.uk/economy/governmentpublicsectorandtaxes/publicspending/datasets/esatable25quarterlynonfinancialaccountsofgeneralgovernment
- UK ERI (XUQAB82):
https://www.bankofengland.co.uk/boeapps/database/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1987&TD=12&TM=Aug&TY=2025&FNY=&CSVF=TT&html.x=160&html.y=25&C=IIY&Filter=N
- LFS (Pop aged 16-64):  
https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/timeseries/lf2o/lms
- GDP (SA) and defaltor rate:  
https://www.ons.gov.uk/economy/grossdomesticproductgdp/datasets/secondestimateofgdp/current
https://www.ons.gov.uk/economy/grossdomesticproductgdp/bulletins/quarterlynationalaccounts/latest#data-on-gdp-quarterly-national-accounts
- interest rate (SR. This is dates of changes to the policy rate. Have interpolated to get quarterly data):  
https://www.bankofengland.co.uk/monetary-policy/the-interest-rate-bank-rate
- 3 month interest rate:  
https://fred.stlouisfed.org/series/IR3TIB01GBM156N

## Data Plots

```{r Seasonally Adjusted Fiscal TS}

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = Revenue_SA, color = "Revenue"), size = 1) +
  geom_line(aes(y = Expenditure_SA, color = "Expenditure"), size = 1) +
  labs(
    x = "Date ID",
    y = "Amount (? in millions)",
    title = "Seasonally Adjusted Revenue and Expenditure",
    color = "Legend"
  ) +
  scale_color_manual(values = c("Revenue" = "blue", "Expenditure" = "red")) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  )


```


```{r Exports TS}

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = Exports, color = "Exports"), size = 1) +
  labs(
    x = "Date ID",
    y = "Amount (? in millions)",
    title = "Exports Over Time",
    color = "Legend"
  ) +
  scale_color_manual(values = c("Exports" = "red")) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  )


```
```{r ERI TS}

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = ERI, color = "ERI"), size = 1) +
  labs(
    x = "Date ID",
    y = "Amount (? in millions)",
    title = "Exchange Rate Index Over Time",
    color = "Legend"
  ) +
  scale_color_manual(values = c("ERI" = "red")) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  )


```



```{r GDP TS}

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = GDP, color = "GDP"), size = 1) +
  labs(
    x = "Date ID",
    y = "Amount (? in millions)",
    title = "GDP Over Time",
    color = "Legend"
  ) +
  scale_color_manual(values = c("GDP" = "red")) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(face = "bold")
  )


```


```{r Deflator TS, message=FALSE, warning=FALSE}

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = Deflator), color = "blue", size = 1) +
  labs(
    x = "Date ID",
    y = "Deflator",
    title = "Deflator Over Time"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```


```{r Mean SR Rate TS, message=FALSE, warning=FALSE}

# Define scaling factor
scale_factor2 <- max(data$Deflator, na.rm = TRUE) / max(data$mean_SR_Rate, na.rm = TRUE)

ggplot(data, aes(x = Unique_Period)) +
  geom_line(aes(y = mean_SR_Rate * scale_factor2), color = "red", size = 1, linetype = "dashed") +
  scale_y_continuous(
    name = "Mean SR Rate (scaled)",
    sec.axis = sec_axis(~ . / scale_factor2, name = "Mean SR Rate (%)")
  ) +
  labs(
    x = "Date ID",
    title = "Mean Short-Term Interest Rate Over Time"
  ) +
  theme_minimal(base_size = 15) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```



# Technical Appendix 

## VECM Code/ Results

```{r VECM, echo = FALSE}
# library(urca)

# urca::cajolst()

# vec2var
# Perform cointegration testing on the (log) levels variables
ca.jo_result_max_eig <- ca.jo(clean_data2[,-c(1,2)], type = "eigen", ecdet = "none", K =4)

ca.jo_result_trace <- ca.jo(clean_data2[,-c(1,2)], type = "trace", ecdet = "none", K =4)

ca.po_result <- ca.po(clean_data2[,-c(1,2)], demean = "const", type = "Pz")

# ca.lut_result <- cajolst(clean_data2[,-c(1,2)], trend = TRUE, K = 2, season = NULL)



summary(ca.jo_result_max_eig)
summary(ca.jo_result_trace)
# summary(ca.po_result)
# summary(ca.lut_result)

# ca.jo()



# scaling_factor <-  T/(T-N*k)
scaling_factor <-  length(clean_data2$Year)/(length(clean_data2$Year)-6*4)


# Max Eigenvalue test H0 r=0:
# test statistic: 41.78 
# Asymptotic Critical value (5% sig level): 39.43 
# scaling_factor = 1.1875
# Finite sample critical value = 90.39 * scaling_factor = 46.82312
# After applying the scaling factor we no longer reject the null of no cointegration. We now consider the trace test. 

# Trace test H0 r=0:
# test statistic: 115.81 
# Asymptotic Critical value (5% sig level): 90.39 
# scaling_factor = 1.1875
# Finite sample critical value = 90.39 * scaling_factor = 107.3381
# After applying the scaling factor, we no longer have sufficient evidence to reject the null of no cointegration.

vecm <- vec2var(ca.jo_result_max_eig, r = 1)  # r = cointegration rank

# SVECM <- SVAR(vecm, Amat = Amat, estmethod = "direct")


# irf_result_SVECM <- irf(SVECM, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = TRUE)  # Forecast 

LR_mat <- matrix(nrow = k, ncol=k)

svecm <- SVEC(ca.jo_result_max_eig, SR =Amat, LR =LR_mat)

irf_temp <- irf(svecm, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = TRUE) 

# plot(irf_temp)





# plotres(ca.jo_result)



# vecm <- cajorls(ca.jo_result, r = 1)
johansen_test <- cajorls(ca.jo_result_max_eig, r = 1)

# johansen_test

vec2var_model <- vec2var(ca.jo_result_max_eig, r = 1)
vec2var_model

class(reduced_VAR)
class(vec2var_model)


# svar_model <- SVAR(vec2var_model, estmethod = "direct", Amat = Amat)
# irf_result <- irf(svar_model, n.ahead = 10, boot = TRUE, ci = 0.68)
# plot(irf_result)

# arch.test(reduced_VAR)
# serial.test(reduced_VAR)
# normality.test(reduced_VAR)



```


```{r IRFs_Table_vecm}





# Extract IRFs
exp_irf_vecm <- round(irf_temp$irf$log_expenditure[, 3],2)
rev_irf_vecm <- round(irf_temp$irf$log_revenue[, 3],2)

exp_irf_df_vecm <- as.data.frame(
  cbind(
    seq(0,10),
    irf_temp$irf$log_expenditure[, 3],
    irf_temp$Lower$log_expenditure[, 3],
    irf_temp$Upper$log_expenditure[, 3]
  )
)

colnames(exp_irf_df_vecm) <- c("Quarter","Expenditure", "LB", "UB")

rev_irf_df_vecm <- as.data.frame(
  cbind(
    seq(0,10),
    irf_temp$irf$log_revenue[, 3],
    irf_temp$Lower$log_revenue[, 3],
    irf_temp$Upper$log_revenue[, 3]
  )
)

colnames(rev_irf_df_vecm) <- c("Quarter","Revenue", "LB", "UB")




# Define a custom theme for consistency
# custom_theme <- theme_minimal(base_size = 14) +
#   theme(
#     plot.title = element_text(hjust = 0.5, face = "bold"),
#     axis.title = element_text(face = "bold"),
#     panel.grid.minor = element_blank()
#   )
# 





```


```{r Restricted-Model-Expenditure-IRF_vecm, fig.cap="Impulse Response Function of GDP Following a Shock to Expenditure ", fig.align='center'}
# Plot for Expenditure IRF
ggplot(exp_irf_df_vecm, aes(x = Quarter)) +
  geom_ribbon(aes(ymin = LB, ymax = UB), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Expenditure), color = "blue", size = 1.2) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  labs(
    title = "Impulse Response of Government Expenditure",
    x = "Quarter",
    y = "Log-Difference Response"
  ) +
  custom_theme

```

```{r Restricted-Model-Revenue-IRF_vecm, fig.cap="Impulse Response Function of GDP Following a Shock to Expenditure ", fig.align='center'}
# Plot for Expenditure IRF
ggplot(rev_irf_df_vecm, aes(x = Quarter)) +
  geom_ribbon(aes(ymin = LB, ymax = UB), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Revenue), color = "blue", size = 1.2) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  labs(
    title = "Impulse Response of Government Expenditure",
    x = "Quarter",
    y = "Log-Difference Response"
  ) +
  custom_theme

```

## IRFs

### Total Period:

```{r, IRFs}



plot_irf_with_ci <- function(IRF_name) {


  # Extract response and confidence intervals
  irf_data  <- as.data.frame(IRF_name$irf)
  irf_lower <- as.data.frame(IRF_name$Lower)
  irf_upper <- as.data.frame(IRF_name$Upper)

  # Create time index
  irf_data$Time  <- seq_len(nrow(irf_data))
  irf_lower$Time <- irf_data$Time
  irf_upper$Time <- irf_data$Time

  # Reshape to long format
  irf_long   <- pivot_longer(irf_data, cols = -Time, names_to = "Variable", values_to = "IRF")
  lower_long <- pivot_longer(irf_lower, cols = -Time, names_to = "Variable", values_to = "Lower")
  upper_long <- pivot_longer(irf_upper, cols = -Time, names_to = "Variable", values_to = "Upper")

  # Merge and label
  irf_combined <- irf_long %>%
    left_join(lower_long, by = c("Time", "Variable")) %>%
    left_join(upper_long, by = c("Time", "Variable")) %>%
    mutate(
      Shock = sub("\\..*", "", Variable),
      Affected_Var = sub(".*\\.", "", Variable)
    )

  # Plot IRFs by shock
  shock_names <- unique(irf_combined$Shock)

  for (shock in shock_names) {
    p <- irf_combined %>%
      filter(Shock == shock) %>%
      ggplot(aes(x = Time, y = IRF)) +
      geom_line(size = 1.2, color = "#1f77b4") +
      geom_line(aes(y = Lower), linetype = "dashed", color = "#7f7f7f", size = 0.9) +
      geom_line(aes(y = Upper), linetype = "dashed", color = "#7f7f7f", size = 0.9) +
      geom_hline(yintercept = 0, color = "black", size = 1.1) +
      facet_wrap(~ Affected_Var, scales = "free_y") +
      theme_minimal(base_family = "serif") +
      theme(
        plot.title = element_text(size = 16, face = "bold"),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        axis.text = element_text(size = 12),
        panel.grid.major = element_line(color = "gray85"),
        panel.grid.minor = element_blank(),
        strip.text = element_text(size = 14, face = "bold")
      ) +
      labs(
        title = paste("Impulse Response for Shock:", shock),
        x = "Time",
        y = "Response"
      ) +
      scale_y_continuous(expand = expansion(mult = c(0, 0.05)))

    print(p)
  }
}

plot_irf_with_ci(irf_result)
# plot_irf_with_ci(irf_result2)
```

```{r FEVD Output}

plot(FEVD_result)

```

## FEVDs post GFC:
```{r Chow tmp}

library(knitr)

# Create named vector of means
means <- c(
  mean(model_data$dif_log_expenditure, na.rm = TRUE),
  mean(model_data$dif_interest_rate, na.rm = TRUE),
  mean(model_data$dif_log_ERI, na.rm = TRUE),
  mean(model_data$dif_log_GDP, na.rm = TRUE),
  mean(model_data$dif_log_revenue, na.rm = TRUE),
  mean(model_data$dif_log_deflator, na.rm = TRUE)
)

# Variable names
variables <- c(
  "dif_log_expenditure",
  "dif_interest_rate",
  "dif_log_ERI",
  "dif_log_GDP",
  "dif_log_revenue",
  "dif_log_deflator"
)

# Assemble and display
mean_table <- data.frame(Variable = variables, Mean = round(means, 5))
kable(mean_table, align = "lr", caption = "Mean Values of Differenced Variables")


run_var_analysis <- function(model_data_type) {
  # Extract suffix from input name
  input_name <- deparse(substitute(model_data_type))
  suffix <- sub("^[^_]+_", "", input_name)
  suffix <- ifelse(nchar(suffix) == 0, "data", suffix)

  # Split data into pre- and post-2008 samples
  clean_data_a <- model_data_type %>%
    filter(Year < 2008 & complete.cases(.))
  clean_data_b <- model_data_type %>%
    filter(!is.na(Year) & Year >= 2008)

  assign(paste0("clean_data_a_", suffix), clean_data_a, envir = .GlobalEnv)
  assign(paste0("clean_data_b_", suffix), clean_data_b, envir = .GlobalEnv)

  # Choose VAR specification 
    var_a <- VAR(clean_data_a[,-c(1,2)], p = 1, type = "const")
    var_b <- VAR(clean_data_b[,-c(1,2)], p = 1, type = "const")


  assign(paste0("reduced_VAR_a_", suffix), var_a, envir = .GlobalEnv)
  assign(paste0("reduced_VAR_b_", suffix), var_b, envir = .GlobalEnv)

  # Check stability
  print(roots(var_a))
  print(roots(var_b))

  # Recursive identification matrix
  k <- ncol(clean_data_a[,-c(1,2)])
  Amat <- diag(1, k)
  Amat[upper.tri(Amat)] <- NA

  # SVAR estimation
  svar_a <- SVAR(var_a, Amat = Amat, estmethod = "direct")
  svar_b <- SVAR(var_b, Amat = Amat, estmethod = "direct")

  assign(paste0("svar_model_a_", suffix), svar_a, envir = .GlobalEnv)
  assign(paste0("svar_model_b_", suffix), svar_b, envir = .GlobalEnv)

  # IRFs
  irf_a <- irf(svar_a, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = TRUE)
  irf_b <- irf(svar_b, n.ahead = 10, ci = 0.68, boot = 5000, cumulative = TRUE)

  assign(paste0("irf_result_a_", suffix), irf_a, envir = .GlobalEnv)
  assign(paste0("irf_result_b_", suffix), irf_b, envir = .GlobalEnv)

  # FEVDs
  fevd_a <- fevd(svar_a, n.ahead = 10)
  fevd_b <- fevd(svar_b, n.ahead = 10)

  assign(paste0("FEVD_result_a_", suffix), fevd_a, envir = .GlobalEnv)
  assign(paste0("FEVD_result_b_", suffix), fevd_b, envir = .GlobalEnv)

  # Plot FEVDs
  # plot(fevd_a)
  plot(fevd_b)
}

run_var_analysis(model_data)
# run_var_analysis(model_data2)

# run the chow test



# plot_irf_with_ci(irf_result2_data)

# Apply processing and plotting functions

```



```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

rev_irf_df_GFC <- as.data.frame(
  cbind(
    seq(0,10),
    irf_result_post_gfc$irf$dif_log_revenue[, 3],
    irf_result_post_gfc$Lower$dif_log_revenue[, 3],
    irf_result_post_gfc$Upper$dif_log_revenue[, 3]
  )
)

colnames(rev_irf_df_GFC) <- c("Quarter","Revenue", "LB", "UB")


ggplot(rev_irf_df_GFC, aes(x = Quarter)) +
  geom_ribbon(aes(ymin = LB, ymax = UB), fill = "lightblue", alpha = 0.4) +
  geom_line(aes(y = Revenue), color = "blue", size = 1.2) +
  geom_hline(yintercept = 0, linetype = "dotted", color = "black") +
  labs(
    title = "Impulse Response of Government Revenue",
    x = "Period",
    y = "Log-Difference Response"
  ) +
  custom_theme

```

